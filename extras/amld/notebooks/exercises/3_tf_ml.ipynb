{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "aGKSSsFCQJlx"}, "source": ["# Machine Learning using Tensorflow\n", "\n", "In this notebook we will solve two simple exercises using low-level\n", "Tensorflow.\n", "\n", "Table of Contents:\n", "\n", "- [ 1 Iterative division](#1-Iterative-division)\n", "- [ 2 Neural function approximator](#2-Neural-function-approximator)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}, "base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "executionInfo": {"elapsed": 3299, "status": "ok", "timestamp": 1526748015362, "user": {"displayName": "Andreas Steiner", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128", "userId": "115804658599019978174"}, "user_tz": -120}, "id": "pGjRHoxoQJly", "outputId": "28d8fd1b-079c-49b4-c8ab-1a0cde7e8f4e"}, "outputs": [], "source": ["from __future__ import division, print_function\n", "\n", "import time\n", "import tensorflow as tf\n", "import numpy as np\n", "from matplotlib import pyplot\n", "# Always make sure you are using running the expected version.\n", "# There are considerable differences between versions...\n", "tf.__version__"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellView": "form", "colab": {"autoexec": {"startup": false, "wait_interval": 0}}, "colab_type": "code", "id": "R0SdU1yAQJl4"}, "outputs": [], "source": ["#@title show_graph()\n", "\n", "# Note : Tested with Chrome 66 -- might not work with all browsers :-(\n", "\n", "# Let's visualize our graph!\n", "# Tip: to make your graph more readable you can add a\n", "# name=\"...\" parameter to the individual Ops.\n", "\n", "# src: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n", "\n", "import numpy as np\n", "import tensorflow as tf\n", "from IPython.display import clear_output, Image, display, HTML\n", "\n", "def strip_consts(graph_def, max_const_size=32):\n", "    \"\"\"Strip large constant values from graph_def.\"\"\"\n", "    strip_def = tf.GraphDef()\n", "    for n0 in graph_def.node:\n", "        n = strip_def.node.add() \n", "        n.MergeFrom(n0)\n", "        if n.op == 'Const':\n", "            tensor = n.attr['value'].tensor\n", "            size = len(tensor.tensor_content)\n", "            if size > max_const_size:\n", "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n", "    return strip_def\n", "\n", "def show_graph(graph_def, max_const_size=32):\n", "    \"\"\"Visualize TensorFlow graph.\"\"\"\n", "    if hasattr(graph_def, 'as_graph_def'):\n", "        graph_def = graph_def.as_graph_def()\n", "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n", "    code = \"\"\"\n", "        <script>\n", "          function load() {{\n", "            document.getElementById(\"{id}\").pbtxt = {data};\n", "          }}\n", "        </script>\n", "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n", "        <div style=\"height:600px\">\n", "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n", "        </div>\n", "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n", "\n", "    iframe = \"\"\"\n", "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n", "    \"\"\".format(code.replace('\"', '&quot;'))\n", "    display(HTML(iframe))"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "rNxui89xQJl6"}, "source": ["# 1 Iterative division\n", "\n", "Using [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n", "to iteratively solve a division:\n", "We first define our graph `A = X * B` and then we provide both\n", "`A` and `B`, and set an initial guess of `X` to zero. We then\n", "iteratively improve our guess for `X` by minimizing the square of the\n", "difference of `A` (=target) and `X * B`.\n", "\n", "Note that we do not need to specify any gradients -- Tensorflow's\n", "`GradientDescentOptimizer` can simply look at our \"forward graph\" and\n", "then generate a \"backward graph\" starting from our loss that consists\n", "of the gradients!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}}, "colab_type": "code", "id": "z1XBwCefQJl7"}, "outputs": [], "source": ["# Use new graph for this section to keep things tidy.\n", "graph = tf.Graph()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}, "base_uri": "https://localhost:8080/", "height": 641}, "colab_type": "code", "executionInfo": {"elapsed": 666, "status": "ok", "timestamp": 1526748017932, "user": {"displayName": "Andreas Steiner", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128", "userId": "115804658599019978174"}, "user_tz": -120}, "id": "tl89m2h0QJl9", "outputId": "a45ff904-5a05-4412-85a8-45b0429bf544"}, "outputs": [], "source": ["# Define our calculation + loss.\n", "\n", "with graph.as_default():\n", "    B = tf.placeholder(shape=(), dtype=tf.float32, name='B')\n", "    X = tf.Variable(0.0, name='X')\n", "    A = tf.placeholder(shape=(), dtype=tf.float32, name='A')\n", "    # Use overloaded Python operators for convenience.\n", "    # (this will translate to tf.mul(), tf.sub(), and tf.pow())\n", "    loss = (A - X * B) ** 2\n", "    # Let's use the identity Op to add a name to the \"loss\" tensor.\n", "    loss = tf.identity(loss, name='loss')\n", "\n", "    show_graph(tf.get_default_graph())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}, "base_uri": "https://localhost:8080/", "height": 641}, "colab_type": "code", "executionInfo": {"elapsed": 787, "status": "ok", "timestamp": 1526748018765, "user": {"displayName": "Andreas Steiner", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128", "userId": "115804658599019978174"}, "user_tz": -120}, "id": "y6UOU8_6QJmB", "outputId": "c2e1fe71-5231-4c7f-aab4-f2d5c83074da"}, "outputs": [], "source": ["# Magic happens : add optimizer.\n", "\n", "with graph.as_default():\n", "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n", "    # We can now minimize the loss by calling this train_op (many times),\n", "    # which will change the value of \"X\" upon every invocation a bit in such\n", "    # a way that the loss becomes smaller.\n", "    train_op = optimizer.minimize(loss)\n", "\n", "    show_graph(tf.get_default_graph())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}, "base_uri": "https://localhost:8080/", "height": 204}, "colab_type": "code", "executionInfo": {"elapsed": 858, "status": "ok", "timestamp": 1526748019668, "user": {"displayName": "Andreas Steiner", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128", "userId": "115804658599019978174"}, "user_tz": -120}, "id": "eXSG2ItSQJmE", "outputId": "4f6b3b2d-76df-469e-c3c1-0078659ccfe5"}, "outputs": [], "source": ["# Do the computation.\n", "\n", "with tf.Session(graph=graph) as sess:\n", "    # Variables must be initialized before first use.\n", "    tf.global_variables_initializer().run()\n", "    # Feed the same result at every step.\n", "    feed_dict = {B: 2, A: 42}\n", "\n", "    print('X=', sess.run([X]))\n", "    for i in range(10):\n", "        # Update b by calling train_op.\n", "        sess.run([train_op], feed_dict=feed_dict)\n", "        # Print our updated guess for X.\n", "        print('X=', sess.run([X]))"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "YjtZPqRFQJmI"}, "source": ["# 2 Neural function approximator"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "yYPGUoL6QJmI"}, "source": ["Neural networks can be seen as generic function approximators\n", "`y = f(x)` -- in this section we will use a network to approximate\n", "a simple mathematical function with a scalar input.\n", "\n", "The same approach can be used to train arbitrary function (for example\n", "a function where `x` is a dense vector of a megapixel image, and `y` is\n", "a probability distribution over thousands of different object classes).\n", "\n", "A crucial part when designing such a network is a good choice of\n", "\"hyperparameters\" that define *how* the network should be created and\n", "trained."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}}, "colab_type": "code", "id": "T4ZmfjQQQJmJ"}, "outputs": [], "source": ["# Helper class for function approximation. Defining all the code\n", "# in a single class makes it easier to instantiate different\n", "# \"versions\" of the approximator (to approximate different functions\n", "# with different configurations).\n", "\n", "class FunctionApproximator(object):\n", "    \"\"\"A class for function approximation using Tensorflow.\n", "\n", "    This class implements both building the graph and updating the\n", "    variables to better approximate the target function, so that it\n", "    can be instantiated with different functions/configurations.\n", "    \"\"\"\n", "\n", "    def __init__(self, f, hidden_layers):\n", "        \"\"\"Initializes a new function approximator.\n", "        \n", "        Args:\n", "          f: Scalar Python functional that should be approximated.\n", "            Signature: f(x:Number) -> Number\n", "          hidden_layers: Iterable specifying number of units per layer.\n", "        \"\"\"\n", "        self.f = f\n", "        self.hidden_layers = hidden_layers\n", "        # Initialize new graph that only contains Ops needed for\n", "        # function approximation.\n", "        self.graph = tf.Graph()\n", "        with self.graph.as_default():\n", "            self._build()\n", "        self.init()\n", "\n", "    def init(self):\n", "        \"\"\"Initializes weights.\"\"\"\n", "        self.sess = tf.Session(graph=self.graph)\n", "        self.init_op.run(session=self.sess)\n", "\n", "    def _build(self):\n", "        # The values for \"x\" and \"y\" are provided during training.\n", "        # Note the dynamic first dimension (=number of samples in batch).\n", "        self.x = tf.placeholder(shape=(None,), dtype=tf.float32)\n", "        self.y = tf.placeholder(shape=(None,), dtype=tf.float32)\n", "        # Reshape e.g. [1, 2, 3] to [[1], [2], [3]] because this latter\n", "        # shape is required by tf.layers() below.\n", "        x = tf.reshape(self.x, (-1, 1))\n", "        for units in self.hidden_layers:\n", "            # We use tf.layers() helper function to create \"fully connected\n", "            # layers\" where every neuron is connected to every neuron in the\n", "            # previous layer. This helper function takes care of defining\n", "            # variables and initializing them.\n", "            x = tf.layers.dense(inputs=x, units=units, activation=tf.nn.relu)\n", "        # Our predicted \"y\".\n", "        self.y_ = tf.layers.dense(inputs=x, units=1, activation=None)\n", "        # Reshape e.g. [[1], [2], [3]] (by tf.layers()) to [1, 2, 3].\n", "        self.y_ = tf.reshape(self.y_, (-1,))\n", "\n", "        # Compute loss.\n", "        self.loss = tf.reduce_mean((self.y - self.y_)**2)\n", "        # Add Ops for minimizing loss.\n", "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n", "        self.train_op = optimizer.minimize(self.loss)\n", "        self.init_op = tf.global_variables_initializer()\n", "\n", "    def fit(self, x_min, x_max, steps, batch_size, show_progress=False):\n", "        \"\"\"Fits the weights to better approximate the function.\n", "\n", "        This function will generate random samples \"x\" within [x_min, x_max]\n", "        and update the network's weight to better approximate y=f(x).\n", "\n", "        Args:\n", "          x_min: Lower bound for generation of \"x\".\n", "          x_max: Upper bound for generation of \"x\".\n", "          steps: Number of training steps.\n", "          batch_size: Number of samples to train network with in every step.\n", "        \"\"\"\n", "        t0 = time.time()\n", "\n", "        losses = []\n", "        for step in range(steps):\n", "            # Generate samples.\n", "            x_data = np.random.uniform(low=x_min, high=x_max, size=batch_size)\n", "            y_data = self.f(x_data)\n", "            feed_dict = {\n", "                self.x: x_data,\n", "                self.y: y_data\n", "            }\n", "            # Update the weights and get the loss.\n", "            loss_data, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n", "            losses.append(loss_data)\n", "            if show_progress and step % (steps // 10) == 0:\n", "                print('step=%6d loss=%f' % (step, loss_data))\n", "\n", "        dt = time.time() - t0\n", "        return losses, dt\n", "\n", "    def predict(self, x_data):\n", "        \"\"\"Computes approximated function values.\"\"\"\n", "        return self.sess.run([self.y_], {self.x: x_data})[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"autoexec": {"startup": false, "wait_interval": 0}, "base_uri": "https://localhost:8080/", "height": 551}, "colab_type": "code", "executionInfo": {"elapsed": 37496, "status": "ok", "timestamp": 1526748058236, "user": {"displayName": "Andreas Steiner", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128", "userId": "115804658599019978174"}, "user_tz": -120}, "id": "H-MzfoS3QJmL", "outputId": "85f7a58b-5e72-4d56-d0ea-11335af97937"}, "outputs": [], "source": ["# \"hidden_layers\" specifies the number of neurons per layer.\n", "# The network will be trained with batch_size x \"steps\" random datapoints.\n", "\n", "# YOUR ACTION REQUIRED:\n", "# Try to achieve a good function approximation by finding good parameters...\n", "hidden_layers, steps = [5, 5, 5, 5, 5], 1000\n", "\n", "tf_sin = FunctionApproximator(f=np.sin, hidden_layers=hidden_layers)\n", "\n", "# Train the network. Note that we do not directly provide x and f(x) samples,\n", "# but rather the function itself and .fit() will then generate samples from\n", "# the function (unlike scikit learn's .fit() method...)\n", "losses, dt = tf_sin.fit(x_min=-3., x_max=3., steps=steps, batch_size=1000,\n", "                        show_progress=True)\n", "print('%.2f seconds' % dt)\n", "\n", "# Visualize some predictions\n", "x_data = np.random.uniform(low=-3., high=3., size=100)\n", "x_data.sort()\n", "y_data = np.sin(x_data)\n", "y_predictions = tf_sin.predict(x_data)\n", "\n", "pyplot.scatter(x_data, y_data)\n", "pyplot.plot(x_data, y_predictions, 'r-')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dS8wNkf7QJmP"}, "source": ["# A References\n", "\n", "Unfortunately, we don't have time to talk about more ML in this\n", "workshop. Two good online references:\n", "\n", "- http://deeplearningbook.org (by Ian Goodfellow et al) \u2013\u00a0more theoretical\n", "- http://neuralnetworksanddeeplearning.com/ (by Michael Nielson) \u2013\u00a0more hands-on"]}], "metadata": {"colab": {"collapsed_sections": [], "default_view": {}, "name": "3_tf_ml.ipynb", "provenance": [], "version": "0.3.2", "views": {}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "toc": {"nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 1}